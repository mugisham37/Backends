# Prometheus Helm chart values

# Global settings
global:
  evaluation_interval: 1m
  scrape_interval: 15s
  scrape_timeout: 10s

# Alertmanager configuration
alertmanager:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job', 'alertname', 'severity']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack'
      routes:
      - match:
          severity: critical
        receiver: 'pagerduty'
    receivers:
    - name: 'slack'
      slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts'
        send_resolved: true
        title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}'
        text: >-
          {{ range .Alerts }}
            *Alert:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            *Severity:* {{ .Labels.severity }}
            *Details:*
            {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
            {{ end }}
          {{ end }}
    - name: 'pagerduty'
      pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        send_resolved: true
        description: '{{ .CommonLabels.alertname }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          status: '{{ .Status }}'
          instance: '{{ .CommonLabels.instance }}'
          summary: '{{ .CommonAnnotations.summary }}'

# Server configuration
server:
  retention: 15d
  persistentVolume:
    enabled: true
    size: 50Gi
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi

# Pushgateway configuration
pushgateway:
  enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

# Node exporter configuration
nodeExporter:
  enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

# Kube state metrics configuration
kubeStateMetrics:
  enabled: true
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi

# Service monitors
serviceMonitors:
  # API Gateway service monitor
  - name: api-gateway
    selector:
      matchLabels:
        app: api-gateway
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # Auth service monitor
  - name: auth-service
    selector:
      matchLabels:
        app: auth-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # Tenant service monitor
  - name: tenant-service
    selector:
      matchLabels:
        app: tenant-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # User service monitor
  - name: user-service
    selector:
      matchLabels:
        app: user-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # Project service monitor
  - name: project-service
    selector:
      matchLabels:
        app: project-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # Billing service monitor
  - name: billing-service
    selector:
      matchLabels:
        app: billing-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

  # Feature service monitor
  - name: feature-service
    selector:
      matchLabels:
        app: feature-service
    endpoints:
      - port: metrics
        interval: 15s
        path: /metrics
    namespaceSelector:
      matchNames:
        - default

# Alert rules
serverFiles:
  alerts:
    groups:
      - name: SaaS Platform Alerts
        rules:
          # Service availability alerts
          - alert: ServiceDown
            expr: up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service {{ $labels.job }} is down"
              description: "{{ $labels.job }} has been down for more than 1 minute."

          # High error rate alerts
          - alert: HighErrorRate
            expr: sum(rate(http_requests_total{status=~"5.."}[5m])) by (job) / sum(rate(http_requests_total[5m])) by (job) > 0.05
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High error rate for {{ $labels.job }}"
              description: "{{ $labels.job }} has a high HTTP error rate (> 5%)."

          # Slow response time alerts
          - alert: SlowResponseTime
            expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job)) > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Slow response time for {{ $labels.job }}"
              description: "{{ $labels.job }} has a 95th percentile response time greater than 1 second."

          # High CPU usage alerts
          - alert: HighCPUUsage
            expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod) / sum(container_spec_cpu_quota{container!=""} / container_spec_cpu_period{container!=""}) by (pod) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage for {{ $labels.pod }}"
              description: "{{ $labels.pod }} is using more than 80% of its CPU quota."

          # High memory usage alerts
          - alert: HighMemoryUsage
            expr: sum(container_memory_working_set_bytes{container!=""}) by (pod) / sum(container_spec_memory_limit_bytes{container!=""}) by (pod) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage for {{ $labels.pod }}"
              description: "{{ $labels.pod }} is using more than 80% of its memory quota."

          # Database connection pool alerts
          - alert: HighDBConnectionUsage
            expr: sum(db_connections_used) by (service) / sum(db_connections_max) by (service) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High DB connection usage for {{ $labels.service }}"
              description: "{{ $labels.service }} is using more than 80% of its database connection pool."

          # Redis memory usage alerts
          - alert: HighRedisMemoryUsage
            expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High Redis memory usage"
              description: "Redis is using more than 80% of its available memory."

          # Kafka lag alerts
          - alert: HighKafkaLag
            expr: sum(kafka_consumergroup_lag) by (consumergroup) > 1000
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High Kafka lag for {{ $labels.consumergroup }}"
              description: "{{ $labels.consumergroup }} has a lag of more than 1000 messages."

          # Disk space alerts
          - alert: LowDiskSpace
            expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Low disk space on {{ $labels.instance }}"
              description: "{{ $labels.instance }} has less than 10% free disk space."

          # Node memory alerts
          - alert: HighNodeMemoryUsage
            expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High node memory usage on {{ $labels.instance }}"
              description: "{{ $labels.instance }} has more than 90% memory usage."
